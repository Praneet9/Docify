{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision as vs\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "\n",
    "from os import listdir\n",
    "from os.path import splitext\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "from utils.nn_block import DualConv, DownConv, UpConv, OutputConv\n",
    "from utils.image_aug import flip, add_gaussian_noise, add_uniform_noise, change_brightness, normalization2\n",
    "\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = '/data/Data/midv500_data/dataset/images_resized/'\n",
    "MASKS_PATH = '/data/Data/midv500_data/dataset/masks_resized/'\n",
    "MODEL_CHECKPOINT_PATH = '/data/Data/midv500_data/dataset/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.inp = DualConv(n_channels, 64)\n",
    "        \n",
    "        self.down_conv_1 = DownConv(64, 128)\n",
    "        self.down_conv_2 = DownConv(128, 256)\n",
    "        self.down_conv_3 = DownConv(256, 512)\n",
    "        self.down_conv_4 = DownConv(512, 1024)\n",
    "        \n",
    "        self.up_conv_1 = UpConv(1024, 512)\n",
    "        self.up_conv_2 = UpConv(512, 256)\n",
    "        self.up_conv_3 = UpConv(256, 128)\n",
    "        self.up_conv_4 = UpConv(128, 64)\n",
    "        \n",
    "        self.op_conv = OutputConv(64, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.inp(x)\n",
    "        \n",
    "        x2 = self.down_conv_1(x1)\n",
    "        x3 = self.down_conv_2(x2)\n",
    "        x4 = self.down_conv_3(x3)\n",
    "        x5 = self.down_conv_4(x4)\n",
    "        \n",
    "        x6 = self.up_conv_1(x5, x4)\n",
    "        x7 = self.up_conv_2(x6, x3)\n",
    "        x8 = self.up_conv_3(x7, x2)\n",
    "        x9 = self.up_conv_4(x8, x1)\n",
    "        \n",
    "        result = self.op_conv(x9)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, mask_suffix=''):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.mask_suffix = mask_suffix\n",
    "        self.height = 480\n",
    "        self.width = 360\n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def preprocess(self, img, mask):\n",
    "        \n",
    "        # Augmentation\n",
    "        # flip {0: vertical, 1: horizontal, 2: both, 3: none}\n",
    "        flip_num = randint(0, 3)\n",
    "        img = flip(img, flip_num)\n",
    "        mask = flip(mask, flip_num)\n",
    "        \n",
    "        # Noise Determine {0: Gaussian_noise, 1: uniform_noise\n",
    "        if randint(0, 1):\n",
    "            # Gaussian_noise\n",
    "            gaus_sd, gaus_mean = randint(0, 20), 0\n",
    "            img = add_gaussian_noise(img, gaus_mean, gaus_sd)\n",
    "        else:\n",
    "            # uniform_noise\n",
    "            l_bound, u_bound = randint(-20, 0), randint(0, 20)\n",
    "            img = add_uniform_noise(img, l_bound, u_bound)\n",
    "        \n",
    "        # Brightness\n",
    "        pix_add = randint(-20, 20)\n",
    "        img = change_brightness(img, pix_add)\n",
    "        \n",
    "        # Normalize the image\n",
    "        img = normalization2(img, max=1, min=0)\n",
    "        \n",
    "        \n",
    "        # Normalize mask to only 0 and 1\n",
    "        mask = mask/255\n",
    "        # msk_as_np = np.expand_dims(msk_as_np, axis=0)  # add additional dimension\n",
    "        \n",
    "        if len(mask.shape) == 2:\n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "            \n",
    "        # HWC to CHW\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n",
    "        img_file = glob(self.imgs_dir + idx + '.*')\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        \n",
    "        mask = cv2.imread(mask_file[0])\n",
    "        img = cv2.imread(img_file[0])\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = cv2.threshold(mask, 100, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        img, mask = self.preprocess(img, mask)\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BasicDataset(IMAGES_PATH, MASKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, img_dir, mask_dir, checkpoint_dir, epochs=20, lr=0.001, val_split=0.20, batch_size=1):\n",
    "    dataset = BasicDataset(img_dir, mask_dir)\n",
    "    val_samples = int(len(dataset) * val_split)\n",
    "    train_samples = len(dataset) - val_samples\n",
    "    train, val = random_split(dataset, [train_samples, val_samples])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, drop_last=True)\n",
    "    \n",
    "    writer = SummaryWriter(logdir=checkpoint_dir, comment=f'LR_{lr}_BS_{batch_size}')\n",
    "    global_step = 0\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        avg_val_loss = np.inf\n",
    "        \n",
    "        with tqdm(total=train_samples, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                \n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if model.n_classes == 1 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "                \n",
    "                masks_pred = model(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                losses.append(loss.item())\n",
    "                writer.add_scalar('Loss/train', sum(losses)/len(losses), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss': sum(losses)/len(losses)})\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "                \n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "\n",
    "            val_loss = 0\n",
    "            for val_batch in val_loader:\n",
    "                imgs, true_masks = val_batch['image'], val_batch['mask']\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.float32)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask_pred = model(imgs)\n",
    "                \n",
    "                pred = torch.sigmoid(mask_pred)\n",
    "                pred = (pred > 0.5).float()\n",
    "                val_loss += criterion(masks_pred, true_masks).item()\n",
    "            val_score = val_loss / len(val_loader)\n",
    "            val_losses.append(val_score)\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            pbar.set_postfix(**{'loss': sum(losses)/len(losses), 'val_loss': avg_val_loss})\n",
    "        \n",
    "        training_loss.append(sum(losses)/len(losses))\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': training_loss,\n",
    "                'val_loss': validation_loss,\n",
    "                'global_step': global_step\n",
    "            }, checkpoint_dir + str(epoch) + '_model.pth')\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 480, 360]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 480, 360]             128\n",
      "              ReLU-3         [-1, 64, 480, 360]               0\n",
      "            Conv2d-4         [-1, 64, 480, 360]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 480, 360]             128\n",
      "              ReLU-6         [-1, 64, 480, 360]               0\n",
      "          DualConv-7         [-1, 64, 480, 360]               0\n",
      "         MaxPool2d-8         [-1, 64, 240, 180]               0\n",
      "            Conv2d-9        [-1, 128, 240, 180]          73,856\n",
      "      BatchNorm2d-10        [-1, 128, 240, 180]             256\n",
      "             ReLU-11        [-1, 128, 240, 180]               0\n",
      "           Conv2d-12        [-1, 128, 240, 180]         147,584\n",
      "      BatchNorm2d-13        [-1, 128, 240, 180]             256\n",
      "             ReLU-14        [-1, 128, 240, 180]               0\n",
      "         DualConv-15        [-1, 128, 240, 180]               0\n",
      "         DownConv-16        [-1, 128, 240, 180]               0\n",
      "        MaxPool2d-17         [-1, 128, 120, 90]               0\n",
      "           Conv2d-18         [-1, 256, 120, 90]         295,168\n",
      "      BatchNorm2d-19         [-1, 256, 120, 90]             512\n",
      "             ReLU-20         [-1, 256, 120, 90]               0\n",
      "           Conv2d-21         [-1, 256, 120, 90]         590,080\n",
      "      BatchNorm2d-22         [-1, 256, 120, 90]             512\n",
      "             ReLU-23         [-1, 256, 120, 90]               0\n",
      "         DualConv-24         [-1, 256, 120, 90]               0\n",
      "         DownConv-25         [-1, 256, 120, 90]               0\n",
      "        MaxPool2d-26          [-1, 256, 60, 45]               0\n",
      "           Conv2d-27          [-1, 512, 60, 45]       1,180,160\n",
      "      BatchNorm2d-28          [-1, 512, 60, 45]           1,024\n",
      "             ReLU-29          [-1, 512, 60, 45]               0\n",
      "           Conv2d-30          [-1, 512, 60, 45]       2,359,808\n",
      "      BatchNorm2d-31          [-1, 512, 60, 45]           1,024\n",
      "             ReLU-32          [-1, 512, 60, 45]               0\n",
      "         DualConv-33          [-1, 512, 60, 45]               0\n",
      "         DownConv-34          [-1, 512, 60, 45]               0\n",
      "        MaxPool2d-35          [-1, 512, 30, 22]               0\n",
      "           Conv2d-36         [-1, 1024, 30, 22]       4,719,616\n",
      "      BatchNorm2d-37         [-1, 1024, 30, 22]           2,048\n",
      "             ReLU-38         [-1, 1024, 30, 22]               0\n",
      "           Conv2d-39         [-1, 1024, 30, 22]       9,438,208\n",
      "      BatchNorm2d-40         [-1, 1024, 30, 22]           2,048\n",
      "             ReLU-41         [-1, 1024, 30, 22]               0\n",
      "         DualConv-42         [-1, 1024, 30, 22]               0\n",
      "         DownConv-43         [-1, 1024, 30, 22]               0\n",
      "  ConvTranspose2d-44          [-1, 512, 60, 44]       2,097,664\n",
      "           Conv2d-45          [-1, 512, 60, 45]       4,719,104\n",
      "      BatchNorm2d-46          [-1, 512, 60, 45]           1,024\n",
      "             ReLU-47          [-1, 512, 60, 45]               0\n",
      "           Conv2d-48          [-1, 512, 60, 45]       2,359,808\n",
      "      BatchNorm2d-49          [-1, 512, 60, 45]           1,024\n",
      "             ReLU-50          [-1, 512, 60, 45]               0\n",
      "         DualConv-51          [-1, 512, 60, 45]               0\n",
      "           UpConv-52          [-1, 512, 60, 45]               0\n",
      "  ConvTranspose2d-53         [-1, 256, 120, 90]         524,544\n",
      "           Conv2d-54         [-1, 256, 120, 90]       1,179,904\n",
      "      BatchNorm2d-55         [-1, 256, 120, 90]             512\n",
      "             ReLU-56         [-1, 256, 120, 90]               0\n",
      "           Conv2d-57         [-1, 256, 120, 90]         590,080\n",
      "      BatchNorm2d-58         [-1, 256, 120, 90]             512\n",
      "             ReLU-59         [-1, 256, 120, 90]               0\n",
      "         DualConv-60         [-1, 256, 120, 90]               0\n",
      "           UpConv-61         [-1, 256, 120, 90]               0\n",
      "  ConvTranspose2d-62        [-1, 128, 240, 180]         131,200\n",
      "           Conv2d-63        [-1, 128, 240, 180]         295,040\n",
      "      BatchNorm2d-64        [-1, 128, 240, 180]             256\n",
      "             ReLU-65        [-1, 128, 240, 180]               0\n",
      "           Conv2d-66        [-1, 128, 240, 180]         147,584\n",
      "      BatchNorm2d-67        [-1, 128, 240, 180]             256\n",
      "             ReLU-68        [-1, 128, 240, 180]               0\n",
      "         DualConv-69        [-1, 128, 240, 180]               0\n",
      "           UpConv-70        [-1, 128, 240, 180]               0\n",
      "  ConvTranspose2d-71         [-1, 64, 480, 360]          32,832\n",
      "           Conv2d-72         [-1, 64, 480, 360]          73,792\n",
      "      BatchNorm2d-73         [-1, 64, 480, 360]             128\n",
      "             ReLU-74         [-1, 64, 480, 360]               0\n",
      "           Conv2d-75         [-1, 64, 480, 360]          36,928\n",
      "      BatchNorm2d-76         [-1, 64, 480, 360]             128\n",
      "             ReLU-77         [-1, 64, 480, 360]               0\n",
      "         DualConv-78         [-1, 64, 480, 360]               0\n",
      "           UpConv-79         [-1, 64, 480, 360]               0\n",
      "           Conv2d-80          [-1, 1, 480, 360]              65\n",
      "       OutputConv-81          [-1, 1, 480, 360]               0\n",
      "================================================================\n",
      "Total params: 31,043,521\n",
      "Trainable params: 31,043,521\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.98\n",
      "Forward/backward pass size (MB): 2688.22\n",
      "Params size (MB): 118.42\n",
      "Estimated Total Size (MB): 2808.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(n_channels=3, n_classes=1)\n",
    "summary(unet.cuda(), (3, 480, 360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "unet = unet.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   6%|â–Œ         | 981/16800 [03:14<53:09,  4.96img/s, loss=0.219]"
     ]
    }
   ],
   "source": [
    "train_model(unet,\n",
    "            device,\n",
    "            IMAGES_PATH,\n",
    "            MASKS_PATH,\n",
    "            MODEL_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
